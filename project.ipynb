{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fuzzywuzzy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e0f4ca5132f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfuzzywuzzy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfuzz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msematch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExtraction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fuzzywuzzy'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding,Input,LSTM,Dense, Dropout\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.merge import concatenate, add, multiply\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from fuzzywuzzy import fuzz\n",
    "from sematch.nlp import Extraction\n",
    "import distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "DATA_PATH = \"./\"\n",
    "WNL = WordNetLemmatizer()\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "MAX_SENT_LEN = 30\n",
    "CUT_SENT_LEN = 8\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "REPLACE_WORD = \"ttitto\"\n",
    "LAPLACE_ADD = 1e-5\n",
    "NUM_OF_FEATURES = 10\n",
    "NUM_K_FOLDS = 1\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "N_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  I. Present raw data distribution and Clear data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH+\"train.csv\", sep=',')\n",
    "test_df = pd.read_csv(DATA_PATH+\"test.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"dark\")\n",
    "\n",
    "def plot_real_feature(fname):\n",
    "    ix_train = train_df['id']\n",
    "    ix_is_dup = np.where(train_df['is_duplicate'] == 1)[0]\n",
    "    ix_not_dup = np.where(train_df['is_duplicate'] == 0)[0]\n",
    "    fig = plt.figure()\n",
    "    ax1 = plt.subplot2grid((3, 2), (0, 0), colspan=2)\n",
    "    ax2 = plt.subplot2grid((3, 2), (1, 0), colspan=2)\n",
    "    ax3 = plt.subplot2grid((3, 2), (2, 0))\n",
    "    ax4 = plt.subplot2grid((3, 2), (2, 1))\n",
    "    ax1.set_title('Distribution of %s' % fname, fontsize=20)\n",
    "    sns.distplot(train_df.loc[ix_train][fname], \n",
    "                 bins=50, \n",
    "                 ax=ax1)    \n",
    "    sns.distplot(train_df.loc[ix_is_dup][fname], \n",
    "                 bins=50, \n",
    "                 ax=ax2,\n",
    "                 label='is dup')    \n",
    "    sns.distplot(train_df.loc[ix_not_dup][fname], \n",
    "                 bins=50, \n",
    "                 ax=ax2,\n",
    "                 label='not dup')\n",
    "    ax2.legend(loc='upper right', prop={'size': 18})\n",
    "    sns.boxplot(y=fname, \n",
    "                x='is_duplicate', \n",
    "                data=train_df.loc[ix_train], \n",
    "                ax=ax3)\n",
    "    sns.violinplot(y=fname, \n",
    "                   x='is_duplicate', \n",
    "                   data=train_df.loc[ix_train], \n",
    "                   ax=ax4)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Absolute value of difference between q1 length and q2 length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f67c14a4fea2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'q1_len'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'q2_len'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'abs_diff_len1_len2'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'q1_len'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'q2_len'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplot_real_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'abs_diff_len1_len2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "train_df['q1_len'] = train_df['question1'].str.len().astype(np.float32)\n",
    "train_df['q2_len'] = train_df['question2'].str.len().astype(np.float32)\n",
    "train_df['abs_diff_len1_len2'] = np.abs(train_df['q1_len'] - train_df['q2_len']).replace(np.nan, 0)\n",
    "plot_real_feature('abs_diff_len1_len2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['log_abs_diff_len1_len2'] = np.log(train_df['abs_diff_len1_len2'] + 1)\n",
    "plot_real_feature('log_abs_diff_len1_len2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Ratio of  q1 length to q2 length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['ratio_len1_len2'] = train_df['q1_len'].apply(lambda x: x if x > 0.0 else 1.0)/\\\n",
    "                        train_df['q2_len'].apply(lambda x: x if x > 0.0 else 1.0)\n",
    "plot_real_feature('ratio_len1_len2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['log_ratio_len1_len2'] = np.log(train_df['ratio_len1_len2'] + 1)\n",
    "plot_real_feature('log_ratio_len1_len2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Clear Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_string(string):\n",
    "    string = string.lower() \\\n",
    "        .replace(\"won't\", \"will not\").replace(\"'ll\", \" will\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\") \\\n",
    "        .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\") \\\n",
    "        .replace(\"'ve\", \" have\").replace(\"'d\", \" would\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\") \\\n",
    "        .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\") \\\n",
    "        .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \").replace(\"€\", \" euro \") \\\n",
    "        .replace(\",000,000\", \"m\").replace(\",000\", \"k\")\\\n",
    "        .replace(\"=\", \" equal \").replace(\"+\", \" plus \").replace(\"&\", \"and\").replace(\"|\", \"or\")\\\n",
    "        .replace(\"′\", \"'\").replace(\"’\", \"'\")\\\n",
    "        .replace(\"e mail\", \"email\").replace(\"e - mail\", \"email\").replace(\"e-mail\", \" email\")\\\n",
    "        .replace(\" quikly \", \" quickly \").replace(\" unseccessful \", \" unsuccessful \").replace(\" insititute \", \" institute \")\\\n",
    "        .replace(\" programmning \", \" programming \").replace(\" litrate \", \" literate \").replace(\" intially \", \" initially \")\\\n",
    "        .replace(\" demonitization \", \" demonetization \").replace(\" actived \", \" active \").replace(\" begineer \", \" beginner \")\\\n",
    "        .replace(\" connectionn \", \" connection \").replace(\" permantley \", \" permanently \").replace(\" litrate \", \" literate \")\n",
    "    string = re.sub('[“”\\(\\'…\\)\\!\\^\\\"\\.;:,\\-\\?？\\{\\}\\[\\]\\\\/\\*@]', ' ', string)\n",
    "    return string\n",
    "\n",
    "\n",
    "#  preliminarily clear data for extracting features\n",
    "def preprocess_df1(df):\n",
    "    df['question1'] = df['question1'].apply(lambda x : clear_string(str(x)))\n",
    "    df['question2'] = df['question2'].apply(lambda x : clear_string(str(x)))\n",
    "\n",
    "    # discard length less than CUT_SENT_LEN characters\n",
    "    df['q1_len'] = df.question1.apply(lambda x : len(x))\n",
    "    df['q2_len'] = df.question2.apply(lambda x : len(x))\n",
    "\n",
    "    indices = set(df[df['q1_len']<CUT_SENT_LEN].index).union(df[df['q2_len']<CUT_SENT_LEN].index)\n",
    "    df.drop(indices, inplace=True)\n",
    "    df.reset_index()\n",
    "    \n",
    "    # Can drop the character count columns - to save memory\n",
    "    df.drop(['q1_len','q2_len'], inplace=True, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df = preprocess_df1(train_df)\n",
    "test_df = preprocess_df1(test_df)\n",
    "print(\"preprocess_df1 finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_bigram_count(a, b):\n",
    "    a = a.split(\" \")\n",
    "    b = b.split(\" \")\n",
    "    a_set = set()\n",
    "    count = 0\n",
    "    for pair in zip(a[:-1], a[1:]):\n",
    "        a_set.add(pair)\n",
    "    for pair in zip(b[:-1], b[1:]):\n",
    "        if pair in a_set:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_longest_substring(a, b):\n",
    "    common_strs = list(distance.lcsubstrings(a, b))\n",
    "    return 0 if len(common_strs) == 0 else len(common_strs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nouns_extraction(q1, q2):\n",
    "    out_list = []\n",
    "    unigrams_que1 = Extraction().extract_nouns(q1)\n",
    "    unigrams_que2 = Extraction().extract_nouns(q2)\n",
    "    print(unigrams_que1)\n",
    "    print(unigrams_que2)\n",
    "    common_unigrams_len = len(set(unigrams_que1).intersection(set(unigrams_que2)))\n",
    "    common_unigrams_ratio = float(common_unigrams_len) / max(len(set(unigrams_que1).union(set(unigrams_que2))),1)\n",
    "    return common_unigrams_len, common_unigrams_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(q1, q2):\n",
    "    features = {}\n",
    "\n",
    "    q1_words = q1.split()\n",
    "    q2_words = q2.split()\n",
    "\n",
    "    if len(q1_words) == 0 or len(q2_words) == 0:\n",
    "        return features\n",
    "\n",
    "    q1_stops = set([word for word in q1_words if word in STOP_WORDS])\n",
    "    q2_stops = set([word for word in q2_words if word in STOP_WORDS])\n",
    "\n",
    "    common_stop_count = len(q1_stops.intersection(q2_stops))\n",
    "    common_word_count = len(set(q1_words).intersection(set(q2_words)))\n",
    "    common_noun_info = nouns_extraction(q1, q2)\n",
    "    \n",
    "    min_len = min(len(q1_words), len(q2_words))\n",
    "    max_len = max(len(q1_words), len(q2_words))\n",
    "\n",
    "    features[\"common_stop_ratio_min\"] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + LAPLACE_ADD)\n",
    "    features[\"common_stop_ratio_max\"] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + LAPLACE_ADD)\n",
    "    features[\"common_unigram_ratio_min\"] = common_word_count / (min_len + LAPLACE_ADD)\n",
    "    features[\"common_unigram_ratio_max\"] = common_word_count / (max_len + LAPLACE_ADD)\n",
    "    features[\"common_bigram_ratio_min\"] = common_bigram_count(q1, q2) / (min_len + LAPLACE_ADD)\n",
    "    features[\"common_bigram_ratio_max\"] = common_bigram_count(q1, q2) / (max_len + LAPLACE_ADD)\n",
    "    features[\"common_substring_ratio_min\"] = common_longest_substring(q1, q2) / (min_len + LAPLACE_ADD)\n",
    "    features[\"common_substring_ratro_max\"] = common_longest_substring(q1, q2) / (max_len + LAPLACE_ADD)\n",
    "    features[\"common_noun_count\"] = common_noun_info[0]\n",
    "    features[\"common_noun_ratio\"] = common_noun_info[1]\n",
    "    features[\"is_last_word_equal\"] = int(q1_words[-1] == q2_words[-1])\n",
    "    features[\"is_fisrt_word_equal\"] = int(q1_words[0] == q2_words[0])\n",
    "    features[\"len_diff\"] = abs(len(q1_words) - len(q2_words))\n",
    "    features[\"fuzzy_common_ratio\"] = fuzz.QRatio(q1, q2)\n",
    "    features[\"fuzzy_set_common_ratio\"] = fuzz.token_set_ratio(q1, q2)\n",
    "    features[\"fuzzy_sort_common_ratio\"] = fuzz.token_set_ratio(q1, q2)\n",
    "    features[\"fuzzy_partial_common_ratio\"] = fuzz.partial_ratio(q1, q2)\n",
    "    \n",
    "    NUM_OF_FEATURES = len(features.keys())\n",
    "    return features\n",
    "\n",
    "def extract_features(df):\n",
    "    print(\"Calculating... (this may take up to several hours)\")\n",
    "    features = df.apply(lambda x: get_features(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    \n",
    "    for feature in features[0].keys():\n",
    "        f = str(feature)\n",
    "        df[f] = [line[f] for line in features]\n",
    "        print(\"Finished extracting \" + f)\n",
    "    return df\n",
    "\n",
    "def generate_features(df, name):\n",
    "    df = extract_features(df)\n",
    "    dfs = np.split(df, [6], axis=1)\n",
    "    dfs[1].to_csv(DATA_PATH + name, index=False) \n",
    "    df = dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.isfile(DATA_PATH + \"features_train.csv\"):\n",
    "    print(\"------------Extracting features in training set---------------\")\n",
    "    generate_features(train_df, \"features_train.csv\")\n",
    "    print(str(NUM_OF_FEATURES) + \" features have been extracted and saved to features_train.csv\")\n",
    "\n",
    "if not os.path.isfile(DATA_PATH + \"features_test.csv\"):\n",
    "    df = test_df\n",
    "    print(\"------------Extracting features in test set---------------\")\n",
    "    generate_features(test_df, \"features_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Embedding Init based on glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_embedding(file_name, embedding_dim):\n",
    "    embeddings_index = {}\n",
    "    f = open(file_name, encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "#         if len(values) == embedding_dim + 1 and word in top_words:\n",
    "        if len(values) == embedding_dim + 1:\n",
    "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    f.close()\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = read_glove_embedding(DATA_PATH+\"glove.840B.300d.txt\", EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = embeddings.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Lemmatization and Process Scarce Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization 词形还原\n",
    "def cutter_and_replace(word):\n",
    "    if len(word) >= 4:\n",
    "        word = WNL.lemmatize(WNL.lemmatize(word, \"n\"), \"v\")\n",
    "    if word in top_words:\n",
    "        return word\n",
    "    elif word not in STOP_WORDS:\n",
    "        return REPLACE_WORD\n",
    "    return \"\"\n",
    "\n",
    "def preprocess_string(string):\n",
    "    string = ' '.join([cutter_and_replace(w) for w in string.split()])\n",
    "    return string\n",
    "\n",
    "# process data\n",
    "def preprocess_df2(df):\n",
    "    df['question1'] = df['question1'].apply(lambda x : preprocess_string(str(x)))\n",
    "    df['question2'] = df['question2'].apply(lambda x : preprocess_string(str(x)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = preprocess_df2(train_df)\n",
    "test_df = preprocess_df2(test_df)\n",
    "print(\"preprocess_df2 finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "# tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(np.append(train_df['question1'],train_df['question2']))\n",
    "word_index = tokenizer.word_index\n",
    "print(\"tokenizer initialization finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(df, tokenizer):\n",
    "    q1 = pad_sequences(tokenizer.texts_to_sequences(df['question1']), maxlen=MAX_SENT_LEN)\n",
    "    q2 = pad_sequences(tokenizer.texts_to_sequences(df['question2']), maxlen=MAX_SENT_LEN)\n",
    "    return q1, q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_q1, train_q2 = tokenize_data(train_df, tokenizer)\n",
    "train_labels = np.array(train_df.is_duplicate)\n",
    "\n",
    "test_q1, test_q2 = tokenize_data(test_df, tokenizer)\n",
    "print(\"tokenize data and get train data labels finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Create an Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding matrix containing only the words in our vocabulary\n",
    "# If the word does not have a pre-trained embedding, then randomly initialize the embedding\n",
    "embeddings_matrix = np.random.uniform(-0.05, 0.05, size=(len(word_index)+1, EMBEDDING_DIM)) # +1 is because the matrix indices start with 0\n",
    "\n",
    "for word, i in word_index.items(): # i=0 is the embedding for the zero padding\n",
    "    try:\n",
    "        embeddings_vector = embeddings[word]\n",
    "    except KeyError:\n",
    "        embeddings_vector = None\n",
    "    if embeddings_vector is not None:\n",
    "        embeddings_matrix[i] = embeddings_vector\n",
    "        \n",
    "del embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_questions_model():\n",
    "    embedding_layer = Embedding(input_dim=len(word_index)+1,\n",
    "                            output_dim=EMBEDDING_DIM,\n",
    "                            weights=[embeddings_matrix],\n",
    "                            input_length=MAX_SENT_LEN,\n",
    "                            trainable=False,\n",
    "                            mask_zero=False,\n",
    "                            name='embedding_layer')\n",
    "    lstm_layer = LSTM(75, recurrent_dropout=0.2)\n",
    "    question_input_1 = Input(shape=(MAX_SENT_LEN,), dtype=\"int32\")\n",
    "    embedded_1 = embedding_layer(question_input_1)\n",
    "    lstm_1 = lstm_layer(embedded_1)\n",
    "\n",
    "    question_input_2 = Input(shape=(MAX_SENT_LEN,), dtype=\"int32\")\n",
    "    embedded_2 = embedding_layer(question_input_2)\n",
    "    lstm_2 = lstm_layer(embedded_2)\n",
    "    return question_input_1, question_input_2, lstm_1, lstm_2\n",
    "    \n",
    "def get_features_model():\n",
    "    features_input = Input(shape=(train_features.shape[1],), dtype=\"float32\")\n",
    "    features_dense = BatchNormalization()(features_input)\n",
    "    features_dense = Dense(200, activation=\"relu\")(features_dense)\n",
    "    features_dense = Dropout(0.2)(features_dense)\n",
    "    return features_input, features_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_q1, X_val_q1, X_train_q2, X_val_q2, y_train, y_val, X_train_features, X_val_features =  train_test_split(train_q1,\n",
    "                                                                                train_q2,\n",
    "                                                                                train_labels,\n",
    "                                                                               train_features,\n",
    "                                                                                random_state=10, \n",
    "                                                                                test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Model without features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_models_no_features(lstm_q1, lstm_q2):\n",
    "    addition= add([lstm_q1, lstm_q2])\n",
    "    minus_lstm_q2 = Lambda(lambda x: -x)(lstm_q2)\n",
    "    merged = add([lstm_q2, minus_lstm_q2])\n",
    "    merged = multiply([merged, merged])\n",
    "    merged = concatenate([merged, addition])\n",
    "    merged = Dropout(0.4)(merged)\n",
    "    \n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = GaussianNoise(0.1)(merged)\n",
    "    \n",
    "    merged = Dense(150, activation=\"relu\")(merged)\n",
    "    merged = Dropout(0.2)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "    \n",
    "    output_layer = Dense(1, activation=\"sigmoid\")(merged)\n",
    "    return output_layer\n",
    "\n",
    "def get_model_no_features():\n",
    "    q1_input, q2_input, lstm_q1, lstm_q2 = get_questions_model()\n",
    "    output_layer = merge_models_no_features(lstm_q1, lstm_q2)\n",
    "    \n",
    "    model = Model(inputs=[q1_input, q2_input], outputs=output_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_no_features = get_model_no_features()\n",
    "\n",
    "model_no_features.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "fit_result = model_no_features.fit(x = [X_train_q1, X_train_q2], \n",
    "          y = y_train, \n",
    "          batch_size=BATCH_SIZE, \n",
    "          epochs=N_EPOCHS, \n",
    "          validation_data=([X_val_q1, X_val_q2], y_val))\n",
    "\n",
    "predict_no_features(model_no_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_no_features(model):\n",
    "    # predict and save the results\n",
    "    preds = model.predict([test_q1, test_q2], batch_size=BATCH_SIZE, verbose=1)\n",
    "\n",
    "    submission = pd.DataFrame({\"test_id\": test_df[\"test_id\"], \"is_duplicate\": preds.ravel()})\n",
    "    submission.to_csv(\"predictions/preds_no_features\" + str(model_num) + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Model with  Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv(DATA_PATH + \"features_train.csv\", encoding ='latin1')\n",
    "# test_features =  pd.read_csv(DATA_PATH + \"features_test.csv\", encoding ='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b1. Model with Simple Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_models_with_simfeat(lstm_q1, lstm_q2, features_dense):\n",
    "    addition= add([lstm_q1, lstm_q2])\n",
    "    minus_lstm_q2 = Lambda(lambda x: -x)(lstm_q2)\n",
    "    merged = add([lstm_q2, minus_lstm_q2])\n",
    "    merged = multiply([merged, merged])\n",
    "    merged = concatenate([merged, addition])\n",
    "    merged = Dropout(0.4)(merged)\n",
    "    \n",
    "    merged = concatenate([merged, features_dense])\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = GaussianNoise(0.1)(merged)\n",
    "    \n",
    "    merged = Dense(150, activation=\"relu\")(merged)\n",
    "    merged = Dropout(0.2)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "    \n",
    "    output_layer = Dense(1, activation=\"sigmoid\")(merged)\n",
    "    return output_layer\n",
    "\n",
    "def get_model_with_simfeat():\n",
    "    q1_input, q2_input, lstm_q1, lstm_q2 = get_questions_model()\n",
    "    features_input, features_dense = get_features_model()\n",
    "    output_layer = merge_models_with_simfeat(lstm_q1, lstm_q2, features_dense)\n",
    "    \n",
    "    model = Model(inputs=[q1_input, q2_input, features_input], outputs=output_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_simfeat(model):\n",
    "    # predict and save the results\n",
    "    preds = model.predict([test_q1, test_q2, test_features], batch_size=BATCH_SIZE, verbose=1)\n",
    "\n",
    "    submission = pd.DataFrame({\"test_id\": test_df[\"test_id\"], \"is_duplicate\": preds.ravel()})\n",
    "    submission.to_csv(\"predictions/preds_with_features\" + str(model_num) + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_simfeat = get_model_with_simfeat()\n",
    "\n",
    "model_with_simfeat.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "fit_result_with_simfeat = model_with_simfeat.fit(x = [X_train_q1, X_train_q2, X_train_features], \n",
    "          y = y_train, \n",
    "          batch_size=BATCH_SIZE, \n",
    "          epochs=N_EPOCHS, \n",
    "          validation_data=([X_val_q1, X_val_q2, X_val_features], y_val))\n",
    "\n",
    "predict_with_simfeat(model_with_simfeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model_with_simfeat, to_file='model_with_simfeat.png', show_layer_names=True)\n",
    "Image('model_with_simfeat.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Result Presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_result(r, title, figname):\n",
    "    fig, ax1 = plt.subplots()\n",
    "    fig.set_figheight(6)\n",
    "    fig.set_figwidth(8)\n",
    "\n",
    "    data1=r.history[\"acc\"]\n",
    "    data2=r.history[\"val_acc\"]\n",
    "    data1l=r.history[\"loss\"]\n",
    "    data2l=r.history[\"val_loss\"]\n",
    "\n",
    "    color = \"r\"\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.plot(data1, c=color, label=\"Train Accuracy\")\n",
    "    ax1.plot(data2, '--', c=color, label=\"Validation Accuracy\")\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.legend(loc=0)\n",
    "\n",
    "    color2 = \"b\"\n",
    "    ax2 = ax1.twinx()  \n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.plot(data1l, c=color2, label=\"Train Loss\")\n",
    "    ax2.plot(data2l, '--', c=color2, label=\"Validation Loss\")\n",
    "    ax2.tick_params(axis='y', labelcolor=color2)\n",
    "    ax2.legend(loc=1)\n",
    "\n",
    "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "    plt.title(title)\n",
    "    plt.savefig(figname, format=\"pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(fit_result, \"Result on training set\", \"First.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
