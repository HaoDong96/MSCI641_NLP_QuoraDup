{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\h5weng\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\h5weng\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers import Embedding,Input,LSTM,Dense, Dropout\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.merge import concatenate, add, multiply\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import plot_model\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "import distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"./\"\n",
    "WNL = WordNetLemmatizer()\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "MAX_SENT_LEN = 30\n",
    "CUT_SENT_LEN = 8\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "REPLACE_WORD = \"ttitto\"\n",
    "LAPLACE_ADD = 1e-5\n",
    "NUM_OF_FEATURES = 10\n",
    "NUM_K_FOLDS = 1\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "N_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Preprocess Data and Extract Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Clear Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clear_string(string):\n",
    "    string = string.lower() \\\n",
    "        .replace(\"won't\", \"will not\").replace(\"'ll\", \" will\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\") \\\n",
    "        .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\") \\\n",
    "        .replace(\"'ve\", \" have\").replace(\"'d\", \" would\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\") \\\n",
    "        .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\") \\\n",
    "        .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \").replace(\"€\", \" euro \") \\\n",
    "        .replace(\",000,000\", \"m\").replace(\",000\", \"k\")\\\n",
    "        .replace(\"=\", \" equal \").replace(\"+\", \" plus \").replace(\"&\", \"and\").replace(\"|\", \"or\")\\\n",
    "        .replace(\"′\", \"'\").replace(\"’\", \"'\")\\\n",
    "        .replace(\"e mail\", \"email\").replace(\"e - mail\", \"email\").replace(\"e-mail\", \" email\")\\\n",
    "        .replace(\" quikly \", \" quickly \").replace(\" unseccessful \", \" unsuccessful \").replace(\" insititute \", \" institute \")\\\n",
    "        .replace(\" programmning \", \" programming \").replace(\" litrate \", \" literate \").replace(\" intially \", \" initially \")\\\n",
    "        .replace(\" demonitization \", \" demonetization \").replace(\" actived \", \" active \").replace(\" begineer \", \" beginner \")\\\n",
    "        .replace(\" connectionn \", \" connection \").replace(\" permantley \", \" permanently \").replace(\" litrate \", \" literate \")\n",
    "    string = re.sub('[“”\\(\\'…\\)\\!\\^\\\"\\.;:,\\-\\?？\\{\\}\\[\\]\\\\/\\*@]', ' ', string)\n",
    "    return string\n",
    "\n",
    "\n",
    "#  preliminarily clear data for extracting features\n",
    "def preprocess_df1(df):\n",
    "    df['question1'] = df['question1'].apply(lambda x : clear_string(str(x)))\n",
    "    df['question2'] = df['question2'].apply(lambda x : clear_string(str(x)))\n",
    "\n",
    "    # discard length less than CUT_SENT_LEN characters\n",
    "    df['q1_len'] = df.question1.apply(lambda x : len(x))\n",
    "    df['q2_len'] = df.question2.apply(lambda x : len(x))\n",
    "\n",
    "    indices = set(df[df['q1_len']<CUT_SENT_LEN].index).union(df[df['q2_len']<CUT_SENT_LEN].index)\n",
    "    df.drop(indices, inplace=True)\n",
    "    df.reset_index()\n",
    "    \n",
    "    # Can drop the character count columns - to save memory\n",
    "    df.drop(['q1_len','q2_len'], inplace=True, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess_df1 finished\n"
     ]
    }
   ],
   "source": [
    "train_df = preprocess_df1(pd.read_table(DATA_PATH+\"train.csv\", sep=','))\n",
    "test_df = preprocess_df1(pd.read_table(DATA_PATH+\"test.csv\", sep=','))\n",
    "print(\"preprocess_df1 finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_token_features(q1, q2):\n",
    "    token_features = {}\n",
    "\n",
    "    q1_tokens = q1.split()\n",
    "    q2_tokens = q2.split()\n",
    "\n",
    "    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n",
    "        return token_features\n",
    "\n",
    "    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])\n",
    "    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])\n",
    "\n",
    "    q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])\n",
    "    q2_stops = set([word for word in q2_tokens if word in STOP_WORDS])\n",
    "\n",
    "    common_word_count = len(q1_words.intersection(q2_words))\n",
    "    common_stop_count = len(q1_stops.intersection(q2_stops))\n",
    "    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n",
    "\n",
    "    token_features[\"common_stop_ratio_min\"] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + LAPLACE_ADD)\n",
    "    token_features[\"common_stop_ratio_max\"] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + LAPLACE_ADD)\n",
    "    token_features[\"common_ratio_min\"] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + LAPLACE_ADD)\n",
    "    token_features[\"common_ratio_max\"] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + LAPLACE_ADD)\n",
    "    token_features[\"is_last_word_equal\"] = int(q1_tokens[-1] == q2_tokens[-1])\n",
    "    token_features[\"is_fisrt_word_equal\"] = int(q1_tokens[0] == q2_tokens[0])\n",
    "    token_features[\"len_diff\"] = abs(len(q1_tokens) - len(q2_tokens))\n",
    "    token_features[\"avg_length\"] = (len(q1_tokens) + len(q2_tokens))/2\n",
    "    \n",
    "    NUM_OF_FEATURES = len(token_features.keys())\n",
    "    return token_features\n",
    "\n",
    "def extract_features(df):\n",
    "    token_features = df.apply(lambda x: get_token_features(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    \n",
    "    print(token_features)\n",
    "    for feature in token_features.keys():\n",
    "        df[feature] = [line[feature] for line in token_features.items()]\n",
    "        print(\"Finished extracting \" + feature)\n",
    "\n",
    "    df[\"token_set_ratio\"]       = df.apply(lambda x: fuzz.token_set_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    df[\"token_sort_ratio\"]      = df.apply(lambda x: fuzz.token_sort_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    df[\"fuzz_ratio\"]            = df.apply(lambda x: fuzz.QRatio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    df[\"fuzz_partial_ratio\"]    = df.apply(lambda x: fuzz.partial_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    print(\"fuzzy features finished\")\n",
    "    return df\n",
    "\n",
    "def generate_features(df, name):\n",
    "    df = extract_features(df)\n",
    "    dfs = np.split(df, [6], axis=1)\n",
    "    dfs[1].to_csv(DATA_PATH + name, index=False) \n",
    "    df = dfs[0]\n",
    "    print(\"save into file finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         {'common_stop_ratio_min': 0.9999983333361112, ...\n",
      "1         {'common_stop_ratio_min': 0.7499981250046875, ...\n",
      "2         {'common_stop_ratio_min': 0.39999920000160005,...\n",
      "3         {'common_stop_ratio_min': 0.0, 'common_stop_ra...\n",
      "4         {'common_stop_ratio_min': 0.9999950000249999, ...\n",
      "5         {'common_stop_ratio_min': 0.8888879012356653, ...\n",
      "6         {'common_stop_ratio_min': 0.0, 'common_stop_ra...\n",
      "7         {'common_stop_ratio_min': 0.5999988000024, 'co...\n",
      "8         {'common_stop_ratio_min': 0.9999975000062501, ...\n",
      "9         {'common_stop_ratio_min': 0.3333322222259259, ...\n",
      "10        {'common_stop_ratio_min': 0.49999750001249993,...\n",
      "11        {'common_stop_ratio_min': 0.5999988000024, 'co...\n",
      "12        {'common_stop_ratio_min': 0.6666644444518518, ...\n",
      "13        {'common_stop_ratio_min': 0.9999966666777778, ...\n",
      "14        {'common_stop_ratio_min': 0.9999990909099173, ...\n",
      "15        {'common_stop_ratio_min': 0.16666638888935187,...\n",
      "16        {'common_stop_ratio_min': 0.9999950000249999, ...\n",
      "17        {'common_stop_ratio_min': 0.24999937500156252,...\n",
      "18        {'common_stop_ratio_min': 0.19999960000080003,...\n",
      "19        {'common_stop_ratio_min': 0.9999975000062501, ...\n",
      "20        {'common_stop_ratio_min': 0.49999750001249993,...\n",
      "21        {'common_stop_ratio_min': 0.49999875000312505,...\n",
      "22        {'common_stop_ratio_min': 0.49999875000312505,...\n",
      "23        {'common_stop_ratio_min': 0.0, 'common_stop_ra...\n",
      "24        {'common_stop_ratio_min': 0.0, 'common_stop_ra...\n",
      "25        {'common_stop_ratio_min': 0.9999987500015626, ...\n",
      "26        {'common_stop_ratio_min': 0.9999950000249999, ...\n",
      "27        {'common_stop_ratio_min': 0.0, 'common_stop_ra...\n",
      "28        {'common_stop_ratio_min': 0.9999966666777778, ...\n",
      "29        {'common_stop_ratio_min': 0.7499981250046875, ...\n",
      "                                ...                        \n",
      "404260    {'common_stop_ratio_min': 0.3333322222259259, ...\n",
      "404261    {'common_stop_ratio_min': 0.9999980000040001, ...\n",
      "404262    {'common_stop_ratio_min': 0.7499981250046875, ...\n",
      "404263    {'common_stop_ratio_min': 0.39999920000160005,...\n",
      "404264    {'common_stop_ratio_min': 0.19999960000080003,...\n",
      "404265    {'common_stop_ratio_min': 0.24999937500156252,...\n",
      "404266    {'common_stop_ratio_min': 0.7999984000032001, ...\n",
      "404267    {'common_stop_ratio_min': 0.7499981250046875, ...\n",
      "404268    {'common_stop_ratio_min': 0.19999960000080003,...\n",
      "404269    {'common_stop_ratio_min': 0.0, 'common_stop_ra...\n",
      "404270    {'common_stop_ratio_min': 0.9999980000040001, ...\n",
      "404271    {'common_stop_ratio_min': 0.16666638888935187,...\n",
      "404272    {'common_stop_ratio_min': 0.8749989062513672, ...\n",
      "404273    {'common_stop_ratio_min': 0.5999988000024, 'co...\n",
      "404274    {'common_stop_ratio_min': 0.5999988000024, 'co...\n",
      "404275    {'common_stop_ratio_min': 0.5999988000024, 'co...\n",
      "404276    {'common_stop_ratio_min': 0.9999950000249999, ...\n",
      "404277    {'common_stop_ratio_min': 0.49999750001249993,...\n",
      "404278    {'common_stop_ratio_min': 0.9999975000062501, ...\n",
      "404279    {'common_stop_ratio_min': 0.571427755103207, '...\n",
      "404280    {'common_stop_ratio_min': 0.9999980000040001, ...\n",
      "404281    {'common_stop_ratio_min': 0.9999983333361112, ...\n",
      "404282    {'common_stop_ratio_min': 0.49999750001249993,...\n",
      "404283    {'common_stop_ratio_min': 0.49999875000312505,...\n",
      "404284    {'common_stop_ratio_min': 0.9999966666777778, ...\n",
      "404285    {'common_stop_ratio_min': 0.9999980000040001, ...\n",
      "404286    {'common_stop_ratio_min': 0.5999988000024, 'co...\n",
      "404287    {'common_stop_ratio_min': 0.9999950000249999, ...\n",
      "404288    {'common_stop_ratio_min': 0.12499984375019532,...\n",
      "404289    {'common_stop_ratio_min': 0.9999980000040001, ...\n",
      "Length: 404200, dtype: object\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "must be str, not numpy.int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-589c9552f842>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"features_train.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mgenerate_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"features_train.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"features_test.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-40ae24b1fa37>\u001b[0m in \u001b[0;36mgenerate_features\u001b[1;34m(df, name)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgenerate_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m     \u001b[0mdfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0mdfs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-40ae24b1fa37>\u001b[0m in \u001b[0;36mextract_features\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtoken_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtoken_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Finished extracting \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfeature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"token_set_ratio\"\u001b[0m\u001b[1;33m]\u001b[0m       \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfuzz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken_set_ratio\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"question1\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"question2\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: must be str, not numpy.int64"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if not os.path.isfile(DATA_PATH + \"features_train.csv\"):\n",
    "    generate_features(train_df, \"features_train.csv\")\n",
    "\n",
    "if not os.path.isfile(DATA_PATH + \"features_test.csv\"):\n",
    "    df = test_df\n",
    "    generate_features(test_df, \"features_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Embedding Init based on glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_glove_embedding(file_name, embedding_dim):\n",
    "    embeddings_index = {}\n",
    "    f = open(file_name, encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "#         if len(values) == embedding_dim + 1 and word in top_words:\n",
    "        if len(values) == embedding_dim + 1:\n",
    "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    f.close()\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = read_glove_embedding(DATA_PATH+\"glove.840B.300d.txt\", EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_words = embeddings.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Lemmatization and Process Scarce Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lemmatization 词形还原\n",
    "def cutter_and_replace(word):\n",
    "    if len(word) >= 4:\n",
    "        word = WNL.lemmatize(WNL.lemmatize(word, \"n\"), \"v\")\n",
    "    if word in top_words:\n",
    "        return word\n",
    "    elif word not in STOP_WORDS:\n",
    "        return REPLACE_WORD\n",
    "    return \"\"\n",
    "\n",
    "def preprocess_string(string):\n",
    "    string = ' '.join([cutter_and_replace(w) for w in string.split()])\n",
    "    return string\n",
    "\n",
    "# process data\n",
    "def preprocess_df2(df):\n",
    "    df['question1'] = df['question1'].apply(lambda x : preprocess_string(str(x)))\n",
    "    df['question2'] = df['question2'].apply(lambda x : preprocess_string(str(x)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = preprocess_df2(train_df)\n",
    "test_df = preprocess_df2(test_df)\n",
    "print(\"preprocess_df2 finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "# tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(np.append(train_df['question1'],train_df['question2']))\n",
    "word_index = tokenizer.word_index\n",
    "print(\"tokenizer initialization finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_data(df, tokenizer):\n",
    "    q1 = pad_sequences(tokenizer.texts_to_sequences(df['question1']), maxlen=MAX_SENT_LEN)\n",
    "    q2 = pad_sequences(tokenizer.texts_to_sequences(df['question2']), maxlen=MAX_SENT_LEN)\n",
    "    return q1, q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_q1, train_q2 = tokenize_data(train_df, tokenizer)\n",
    "train_labels = np.array(train_df.is_duplicate)\n",
    "\n",
    "test_q1, test_q2 = tokenize_data(test_df, tokenizer)\n",
    "print(\"tokenize data and get train data labels finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f. Create an Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an embedding matrix containing only the words in our vocabulary\n",
    "# If the word does not have a pre-trained embedding, then randomly initialize the embedding\n",
    "embeddings_matrix = np.random.uniform(-0.05, 0.05, size=(len(word_index)+1, EMBEDDING_DIM)) # +1 is because the matrix indices start with 0\n",
    "\n",
    "for word, i in word_index.items(): # i=0 is the embedding for the zero padding\n",
    "    try:\n",
    "        embeddings_vector = embeddings[word]\n",
    "    except KeyError:\n",
    "        embeddings_vector = None\n",
    "    if embeddings_vector is not None:\n",
    "        embeddings_matrix[i] = embeddings_vector\n",
    "        \n",
    "del embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_features = pd.read_csv(DATA_PATH + \"features_train.csv\", encoding ='latin1')\n",
    "# test_features =  pd.read_csv(DATA_PATH + \"features_test.csv\", encoding ='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_questions_model():\n",
    "    embedding_layer = Embedding(input_dim=len(word_index)+1,\n",
    "                            output_dim=EMBEDDING_DIM,\n",
    "                            weights=[embeddings_matrix],\n",
    "                            input_length=MAX_SENT_LEN,\n",
    "                            trainable=False,\n",
    "                            mask_zero=False,\n",
    "                            name='embedding_layer')\n",
    "    lstm_layer = LSTM(75, recurrent_dropout=0.2)\n",
    "    question_input_1 = Input(shape=(MAX_SENT_LEN,), dtype=\"int32\")\n",
    "    embedded_1 = embedding_layer(question_input_1)\n",
    "    lstm_1 = lstm_layer(embedded_1)\n",
    "\n",
    "    question_input_2 = Input(shape=(MAX_SENT_LEN,), dtype=\"int32\")\n",
    "    embedded_2 = embedding_layer(question_input_2)\n",
    "    lstm_2 = lstm_layer(embedded_2)\n",
    "    return question_input_1, question_input_2, lstm_1, lstm_2\n",
    "    \n",
    "def get_features_model():\n",
    "    features_input = Input(shape=(train_features.shape[1],), dtype=\"float32\")\n",
    "    features_dense = BatchNormalization()(features_input)\n",
    "    features_dense = Dense(200, activation=\"relu\")(features_dense)\n",
    "    features_dense = Dropout(0.2)(features_dense)\n",
    "    return features_input, features_dense\n",
    "\n",
    "def merge_models(lstm_q1, lstm_q2, features_dense):\n",
    "    addition= add([lstm_q1, lstm_q2])\n",
    "    minus_lstm_q2 = Lambda(lambda x: -x)(lstm_q2)\n",
    "    merged = add([lstm_q2, minus_lstm_q2])\n",
    "    merged = multiply([merged, merged])\n",
    "    merged = concatenate([merged, addition])\n",
    "    merged = Dropout(0.4)(merged)\n",
    "    \n",
    "    merged = concatenate([merged, features_dense])\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = GaussianNoise(0.1)(merged)\n",
    "    \n",
    "    merged = Dense(150, activation=\"relu\")(merged)\n",
    "    merged = Dropout(0.2)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "    \n",
    "    output_layer = Dense(1, activation=\"sigmoid\")(merged)\n",
    "    return output_layer\n",
    "\n",
    "def get_model():\n",
    "    q1_input, q2_input, lstm_q1, lstm_q2 = get_questions_model()\n",
    "    features_input, features_dense = get_features_model()\n",
    "    output_layer = merge_models(lstm_q1, lstm_q2, features_dense)\n",
    "    \n",
    "    model = Model(inputs=[q1_input, q2_input, features_input], outputs=output_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(model):\n",
    "    # predict and save the results\n",
    "    preds = model.predict([test_q1, test_q2, test_features], batch_size=BATCH_SIZE, verbose=1)\n",
    "\n",
    "    submission = pd.DataFrame({\"test_id\": test_df[\"test_id\"], \"is_duplicate\": preds.ravel()})\n",
    "    submission.to_csv(\"predictions/preds\" + str(model_num) + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# skf = StratifiedKFold(n_splits=NUM_K_FOLDS, shuffle=True)\n",
    "# model_num = 0\n",
    "# # Stratified k-fold to split train data and val data for Cross-validation\n",
    "# for train_id, val_id in skf.split(train_q1, train_labels):\n",
    "#     print(\"MODEL:\", model_num)\n",
    "    \n",
    "#     train_data_q1 = train_q1[train_id]\n",
    "#     train_data_q2 = train_q2[train_id]\n",
    "#     train_data_labels = train_labels[train_id]\n",
    "#     train_data_features = train_features.values[train_id]\n",
    "    \n",
    "#     val_data_q1 = train_q1[val_id]\n",
    "#     val_data_q2 = train_q2[val_id]\n",
    "#     val_data_labels = train_labels[val_id]\n",
    "#     val_data_features = train_features.values[val_id]\n",
    "    \n",
    "#     # get the model\n",
    "#     model = get_model()\n",
    "#     model.compile(loss='binary_crossentropy',\n",
    "#                   optimizer='nadam')\n",
    "    \n",
    "#     # callbacks\n",
    "#     early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "#     best_model_path = \"best_model\"+str(model_num) +\".h5\"\n",
    "#     model_checkpoint = ModelCheckpoint(best_model_path, save_best_only=True, save_weights_only=True)\n",
    "    \n",
    "#     # fit the model\n",
    "#     fit_result = model.fit(x = [train_data_q1, train_data_q2, train_data_features], \n",
    "#                            y = train_data_labels,\n",
    "#                            validation_data = ([val_data_q1, val_data_q2, val_data_features], val_data_labels),\n",
    "#                            batch_size=BATCH_SIZE, \n",
    "#                            epochs=N_EPOCHS, \n",
    "#                            callbacks=[early_stopping, model_checkpoint]\n",
    "#                           )\n",
    "#     model.load_weights(best_model_path)\n",
    "#     print(model_count, \"validation loss:\", min(hist.history[\"val_loss\"]))\n",
    "\n",
    "# #     predict(model)\n",
    "\n",
    "#     model_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_q1, X_val_q1, X_train_q2, X_val_q2, y_train, y_val, X_train_features, X_val_features =  train_test_split(train_q1,\n",
    "                                                                                train_q2,\n",
    "                                                                                train_labels,\n",
    "                                                                               train_features,\n",
    "                                                                                random_state=10, \n",
    "                                                                                test_size=0.1)\n",
    "model = get_model()\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "fit_result = model.fit(x = [X_train_q1, X_train_q2, X_train_features], \n",
    "          y = y_train, \n",
    "          batch_size=BATCH_SIZE, \n",
    "          epochs=N_EPOCHS, \n",
    "          validation_data=([X_val_q1, X_val_q2, X_val_features], y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_31 (InputLayer)           (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer (Embedding)     (None, 30, 300)      16141800    input_30[0][0]                   \n",
      "                                                                 input_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_11 (LSTM)                  (None, 75)           112800      embedding_layer[0][0]            \n",
      "                                                                 embedding_layer[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_30 (InputLayer)           (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 75)           0           lstm_11[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 75)           0           lstm_11[1][0]                    \n",
      "                                                                 lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_32 (InputLayer)           (None, 15)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "multiply_6 (Multiply)           (None, 75)           0           add_12[0][0]                     \n",
      "                                                                 add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 75)           0           lstm_11[0][0]                    \n",
      "                                                                 lstm_11[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 15)           60          input_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 150)          0           multiply_6[0][0]                 \n",
      "                                                                 add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 200)          3200        batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 150)          0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 200)          0           dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 350)          0           dropout_17[0][0]                 \n",
      "                                                                 dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 350)          1400        concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_6 (GaussianNoise (None, 350)          0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 150)          52650       gaussian_noise_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 150)          0           dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 150)          600         dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 1)            151         batch_normalization_21[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 16,312,661\n",
      "Trainable params: 169,831\n",
      "Non-trainable params: 16,142,830\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Result Presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_result(r, title, figname):\n",
    "    fig, ax1 = plt.subplots()\n",
    "    fig.set_figheight(6)\n",
    "    fig.set_figwidth(8)\n",
    "\n",
    "    data1=r.history[\"acc\"]\n",
    "    data2=r.history[\"val_acc\"]\n",
    "    data1l=r.history[\"loss\"]\n",
    "    data2l=r.history[\"val_loss\"]\n",
    "\n",
    "    color = \"r\"\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.plot(data1, c=color, label=\"Train Accuracy\")\n",
    "    ax1.plot(data2, '--', c=color, label=\"Validation Accuracy\")\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.legend(loc=0)\n",
    "\n",
    "    color2 = \"b\"\n",
    "    ax2 = ax1.twinx()  \n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.plot(data1l, c=color2, label=\"Train Loss\")\n",
    "    ax2.plot(data2l, '--', c=color2, label=\"Validation Loss\")\n",
    "    ax2.tick_params(axis='y', labelcolor=color2)\n",
    "    ax2.legend(loc=1)\n",
    "\n",
    "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "    plt.title(title)\n",
    "    plt.savefig(figname, format=\"pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_result(fit_result, \"Result on training set\", \"First.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
