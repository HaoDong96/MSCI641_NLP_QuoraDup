{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers import Embedding,Input,LSTM,Concatenate,Dense\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./\"\n",
    "WNL = WordNetLemmatizer()\n",
    "MAX_SENT_LEN = 15\n",
    "EMBEDDING_DIM = 300\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization 词形还原\n",
    "def cutter(word):\n",
    "    if len(word) < 4:\n",
    "        return word\n",
    "    return WNL.lemmatize(WNL.lemmatize(word, \"n\"), \"v\")\n",
    "\n",
    "def preprocess(string):\n",
    "    string = string.lower().replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\") \\\n",
    "        .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\") \\\n",
    "        .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\") \\\n",
    "        .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\") \\\n",
    "        .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\") \\\n",
    "        .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \") \\\n",
    "        .replace(\"€\", \" euro \").replace(\"'ll\", \" will\").replace(\"=\", \" equal \").replace(\"+\", \" plus \")\n",
    "    string = re.sub('[“”\\(\\'…\\)\\!\\^\\\"\\.;:,\\-\\?？\\{\\}\\[\\]\\\\/\\*@]', ' ', string)\n",
    "    string = re.sub(r\"([0-9]+)000000\", r\"\\1m\", string)\n",
    "    string = re.sub(r\"([0-9]+)000\", r\"\\1k\", string)\n",
    "#     string = ' '.join([cutter(w) for w in string.split()])\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df):\n",
    "    df['question1'] = df['question1'].apply(lambda x : preprocess(str(x)))\n",
    "    df['question2'] = df['question2'].apply(lambda x : preprocess(str(x)))\n",
    "\n",
    "    # discard length less than 10 characters\n",
    "    df['q1_len'] = df.question1.apply(lambda x : len(x))\n",
    "    df['q2_len'] = df.question2.apply(lambda x : len(x))\n",
    "\n",
    "    # Questions having lesser than 10 characters can be discarded. \n",
    "    indices = set(df[df['q1_len']<10].index).union(df[df['q2_len']<10].index)\n",
    "\n",
    "    # Can drop the character count columns - to save memory\n",
    "    df.drop(['q1_len','q2_len'], inplace=True, axis=1)\n",
    "\n",
    "    df.drop(indices, inplace=True)\n",
    "    df.reset_index()\n",
    "\n",
    "    ## cut every question length to MAX_SENT_LENTH\n",
    "    # df['q1_wc'] = df.question1.apply(lambda x : len(x.split()))\n",
    "    # df['q2_wc'] = df.question2.apply(lambda x : len(x.split()))\n",
    "    # MAX_SENT_LEN = int(max(np.percentile(df.q1_wc, 80),np.percentile(df.q2_wc, 80)))\n",
    "    \n",
    "    \n",
    "def tokenize_data(df, tokenizer):\n",
    "    data_1 = pad_sequences(tokenizer.texts_to_sequences(df.question1), maxlen=MAX_SENT_LEN)\n",
    "    data_2 = pad_sequences(tokenizer.texts_to_sequences(df.question2), maxlen=MAX_SENT_LEN)\n",
    "    return data_1, data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_table(DATA_PATH+\"train.csv\", sep=',')\n",
    "test_df = pd.read_table(DATA_PATH+\"test.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_df(train_df)\n",
    "preprocess_df(test_df)\n",
    "print(\"preprocess_df finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "# tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(np.append(train_df.question1,train_df.question2))\n",
    "word_index = tokenizer.word_index\n",
    "print(\"tokenizer initialization finished\")\n",
    "\n",
    "\n",
    "train_data_1 = train_data_2 = train_labels =  np.array([])\n",
    "train_data_1, train_data_2 = tokenize_data(train_df, tokenizer)\n",
    "train_labels = np.array(train_df.is_duplicate)\n",
    "\n",
    "test_data_1 = test_data_2 = np.array([])\n",
    "test_data_1, test_data_2 = tokenize_data(test_df, tokenizer)\n",
    "print(\"get_data_labels finished\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_embedding(file_name, embedding_dim):\n",
    "    embeddings_index = {}\n",
    "    f = open(file_name)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "#         if len(values) == embedding_dim + 1 and word in top_words:\n",
    "        if len(values) == embedding_dim + 1:\n",
    "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    f.close()\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = read_glove_embedding(DATA_PATH+\"glove.840B.300d.txt\", EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding matrix containing only the word's in our vocabulary\n",
    "# If the word does not have a pre-trained embedding, then randomly initialize the embedding\n",
    "embeddings_matrix = np.random.uniform(-0.05, 0.05, size=(len(word_index)+1, EMBEDDING_DIM)) # +1 is because the matrix indices start with 0\n",
    "\n",
    "for word, i in word_index.items(): # i=0 is the embedding for the zero padding\n",
    "    try:\n",
    "        embeddings_vector = embeddings[word]\n",
    "    except KeyError:\n",
    "        embeddings_vector = None\n",
    "    if embeddings_vector is not None:\n",
    "        embeddings_matrix[i] = embeddings_vector\n",
    "        \n",
    "# del embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(input_dim=len(word_index)+1,\n",
    "                            output_dim=EMBEDDING_DIM,\n",
    "                            weights=[embeddings_matrix],\n",
    "                            input_length=MAX_SENT_LEN,\n",
    "                            trainable=False,\n",
    "                            mask_zero=False,\n",
    "                            name='embedding_layer')\n",
    "lstm_layer = LSTM(75, recurrent_dropout=0.2)\n",
    "\n",
    "question_input_1 = Input(shape=(MAX_SENT_LEN,), dtype=\"int32\")\n",
    "embedded_1 = embedding_layer(question_input_1)\n",
    "lstm_1 = lstm_layer(embedded_1)\n",
    "\n",
    "question_input_2 = Input(shape=(MAX_SENT_LEN,), dtype=\"int32\")\n",
    "embedded_2 = embedding_layer(question_input_2)\n",
    "lstm_2 = lstm_layer(embedded_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = Concatenate(name='q1_q2_concat')([lstm_1, lstm_2])\n",
    "output_prob = Dense(units=1, \n",
    "                    activation='sigmoid', \n",
    "                    name='output_layer')(merged)\n",
    "model = Model(inputs=[question_input_1, question_input_2], outputs=output_prob, name='text_pair_cnn')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_q1, X_val_q1, X_train_q2, X_val_q2, y_train, y_val =  train_test_split(train_data_1,\n",
    "                                                                                train_data_2,\n",
    "                                                                                train_df.is_duplicate,\n",
    "                                                                                random_state=10, \n",
    "                                                                                test_size=0.1)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "model.fit(x = [X_train_q1, X_train_q2], \n",
    "          y = y_train, \n",
    "          batch_size=BATCH_SIZE, \n",
    "          epochs=N_EPOCHS, \n",
    "          validation_data=([X_val_q1, X_val_q2], y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
